A CLI tool to run locally LLMs without needing to configure, install and make sure that the hardware is supported

- `ollama serve` to run server (needs to run in background for other commands to work)
- `ollama run <image name>` to pull and run an LLM model (from https://ollama.ai/library)
